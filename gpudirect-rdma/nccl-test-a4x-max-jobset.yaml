apiVersion: resource.nvidia.com/v1beta1
kind: ComputeDomain
metadata:
  name: nccl-test-compute-domain
spec:
  numNodes: __NUM_NODES__
  channel:
    resourceClaimTemplate:
      name: nccl-test-compute-domain-channel
---
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: allgather
  # labels:
  #   kueue.x-k8s.io/queue-name: a4x
spec:
  ttlSecondsAfterFinished: 1200
  network:
    enableDNSHostnames: true
  replicatedJobs:
    - name: worker
      template:
        spec:
          parallelism: __NUM_NODES__
          completions: __NUM_NODES__
          template:
            spec:
              terminationGracePeriodSeconds: 5
              activeDeadlineSeconds: 3600
              restartPolicy: Never
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              nodeSelector:
                cloud.google.com/gke-accelerator: nvidia-gb300
              tolerations:
              - key: nvidia.com/gpu
                operator: Equal
                value: present
                effect: NoSchedule
              - key: kubernetes.io/arch
                operator: Equal
                value: arm64
                effect: NoSchedule
              setHostnameAsFQDN: true
              volumes:
              # - name: gib
              #   hostPath:
              #     path: /home/kubernetes/bin/gib
              - name: nvidia
                hostPath:
                  path: /home/kubernetes/bin/nvidia
              - name: lib64
                hostPath:
                  path: /lib64
              - name: shared-memory
                emptyDir:
                  medium: "Memory"
                  sizeLimit: 250Gi
              resourceClaims:
              - name: compute-domain-channel
                resourceClaimTemplateName: nccl-test-compute-domain-channel
              containers:
              - name: nccl-test
                stdin: true
                tty: true
                image: us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-gib-a4x-max-arm64@sha256:a0a914c56b8eeb8130ed4d8a5fb811d87826964b4458cbd87740a33bd1e66d33
                securityContext:
                  privileged: true
                env:
                - name: MY_NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                - name: MY_JOB_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.labels['job-name']
                - name: MY_SUBDOMAIN
                  value: "allgather"
                - name: OMPI_ALLOW_RUN_AS_ROOT
                  value: "1"
                - name: OMPI_ALLOW_RUN_AS_ROOT_CONFIRM
                  value: "1"
                - name: N_NODES
                  value: "2"
                - name: LD_LIBRARY_PATH
                  value: /usr/local/nvidia/lib64
                command:
                - bash
                - -c
                - |
                  set -x
                  echo "Starting workload container on ${MY_NODE_NAME} for $N_NODES benchmark"
                  # Install ping
                  apt update -y
                  apt install -y iputils-ping
                  # sleep infinity

                  # Start sshd
                  /scripts/container_entry.sh daemon &

                  # Get helper variables to form all hostnames
                  export NODE_RANK=$JOB_COMPLETION_INDEX

                  # For every worker, wait till online and add to hostfile
                  for i in $(seq 0 $(($N_NODES-1))); do
                    OTHER=${MY_JOB_NAME}-${i}.${MY_SUBDOMAIN}
                    until ssh -p 222 -o StrictHostKeyChecking=no $OTHER hostname; do
                      echo Waiting for ${OTHER}...
                      sleep 10
                    done
                    echo ${OTHER} port=222 slots=4 | tee -a /tmp/hostfile;
                  done

                  cat /tmp/hostfile

                  # Launch from head node
                  if [[ "${NODE_RANK}" -eq "0" ]]; then

                      # World Level = 0x0, Rail Aligned = 0x7
                      export NCCL_TESTS_SPLIT_MASK="0x0";

                      # Force use of libnccl-gib
                      # export NCCL_NET=gIB

                      # Set all the correct libnccl-gib environment variables
                      # source /usr/local/gib/scripts/set_nccl_env.sh

                      # Get all relevant NCCL / env vars to pass to all workers
                      export NCCL_DEBUG_SUBSYS="INIT,NET"
                      export NCCL_DEBUG="INFO"
                      LD_LIBRARY_PATH="/usr/local/gib/lib64:${LD_LIBRARY_PATH}"
                      ENV_VARS=$(echo ${!NCCL*} ${!OMPI*} LD_LIBRARY_PATH PATH | sed 's/ / -x /g')

                      mpirun --hostfile /tmp/hostfile \
                        -x PATH -x LD_LIBRARY_PATH="/usr/local/gib/lib64:${LD_LIBRARY_PATH}" \
                        -x $ENV_VARS \
                        -x NCCL_IB_GID_INDEX=3 \
                        -x NCCL_DEBUG="${NCCL_DEBUG}" \
                        -x NCCL_DEBUG_SUBSYS="${NCCL_DEBUG_SUBSYS}" \
                        -x NCCL_TESTS_SPLIT_MASK="${NCCL_TESTS_SPLIT_MASK}" \
                        -mca plm_rsh_no_tree_spawn 1 \
                        --mca orte_keep_fqdn_hostnames 1 \
                        --mca btl self,tcp \
                        --mca btl_tcp_if_include eth0 \
                        --bind-to none \
                        --mca plm_rsh_agent "ssh -q -o LogLevel=ERROR -o StrictHostKeyChecking=no -p 222" \
                        /third_party/nccl-tests/build/all_gather_perf -b 1K -e 8G -f 2 -g 1 -w 5 --iters 100 -c 1

                  else
                    HEAD_NODE=${MY_JOB_NAME}-0.${MY_SUBDOMAIN}
                    # Wait for head node to exit
                    while ssh -p 222 -o StrictHostKeyChecking=no $HEAD_NODE hostname; do
                      sleep 5
                    done
                  fi

                  exit 0
                volumeMounts:
                - name: nvidia
                  mountPath: /usr/local/nvidia
                # - name: gib
                #   mountPath: /usr/local/gib
                - name: shared-memory
                  mountPath: /dev/shm
                resources:
                  limits:
                    nvidia.com/gpu: 4
                  requests:
                    nvidia.com/gpu: 4
                  claims:
                    - name: compute-domain-channel
              restartPolicy: Never